{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型设计的代码需要用到上一节数据处理的Python类，定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import Linear, Embedding, Conv2D\n",
    "import paddle.nn.functional as F\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MovieLen(object):\n",
    "    def __init__(self, use_poster):\n",
    "        self.use_poster = use_poster\n",
    "        # 声明每个数据文件的路径\n",
    "        usr_info_path = \"/Users/wlin109/Desktop/awesome-DeepLearning/junior_class/chapter-7-Recommendation_System/notebook/data_freelancer/freelancers.xlsx\"\n",
    "        if use_poster:\n",
    "            rating_path = \"./work/ml-1m/new_rating.txt\"\n",
    "        else:\n",
    "            rating_path = \"/Users/wlin109/Desktop/awesome-DeepLearning/junior_class/chapter-7-Recommendation_System/notebook/data_freelancer/post_rating.xlsx\"\n",
    "\n",
    "        job_info_path = \"/Users/wlin109/Desktop/awesome-DeepLearning/junior_class/chapter-7-Recommendation_System/notebook/data_freelancer/jobs.xlsx\"\n",
    "        self.poster_path = \"./work/ml-1m/posters/\"\n",
    "        # 得到电影数据\n",
    "        self.job_info, self.job_city = self.get_job_info(job_info_path)\n",
    "        # 记录电影的最大ID\n",
    "        # self.max_job = np.max([self.movie_cat[k] for k in self.movie_cat])\n",
    "        # self.max_mov_tit = np.max([self.movie_title[k] for k in self.movie_title])\n",
    "        self.max_job_id = np.max(list(map(int, self.job_info.keys())))\n",
    "        self.max_job_city = np.max(list(map(int, self.job_city.values())))\n",
    "        # 记录用户数据的最大ID\n",
    "        self.max_usr_id = 0\n",
    "        # 得到用户数据\n",
    "        self.usr_info, self.usr_title_info, self.usr_state_info = self.get_usr_info(usr_info_path)\n",
    "        self.max_usr_title = np.max([self.usr_title_info[k] for k in self.usr_title_info])\n",
    "        self.max_usr_state = np.max(list(map(int, self.usr_state_info.values())))\n",
    "        # print(self.usr_info)\n",
    "        # 得到评分数据\n",
    "        self.rating_info = self.get_rating_info(rating_path)\n",
    "        # 构建数据集 \n",
    "        self.dataset = self.get_dataset(usr_info=self.usr_info,\n",
    "                                        job_info=self.job_info,\n",
    "                                        rating_info=self.rating_info\n",
    "                                       )\n",
    "        # 划分数据集，获得数据加载器\n",
    "        self.train_dataset = self.dataset[:int(len(self.dataset)*0.9)]\n",
    "        self.valid_dataset = self.dataset[int(len(self.dataset)*0.9):]\n",
    "        print(\"##Total dataset instances: \", len(self.dataset))\n",
    "        print(\"##MovieLens dataset information: \\nusr num: {}\\n\"\n",
    "              \"job num: {}\".format(len(self.usr_info),len(self.job_info)))\n",
    "    # 得到电影数据\n",
    "    def get_job_info(self, path):\n",
    "        # 打开文件，编码方式选择ISO-8859-1，读取所有数据到data中 \n",
    "        df_job = pd.read_excel(open(path, 'rb'),\n",
    "                             sheet_name='summary')\n",
    "        # 建立三个字典，分别用户存放电影所有信息，电影的名字信息、类别信息\n",
    "        job_info, job_city = {}, {}\n",
    "        df_job['Value'].replace(['Min of 1 year','Min of 3 years','Min of 5 years','Min of 7 years','Min of 9 years'],[1,3,5,7,9],inplace= True)\n",
    "        # 对电影名字、类别中不同的单词计数\n",
    "        count_c = 1\n",
    "        \n",
    "        for i in df_job.index:\n",
    "            job_id = df_job['jobid'][i]\n",
    "            \n",
    "            if df_job['WorkLocationCity'][i] not in job_city:\n",
    "                job_city[df_job['WorkLocationCity'][i]] = count_c\n",
    "                count_c +=1\n",
    "            job_info[job_id] = {'job_id': int(job_id),\n",
    "                                'job_city': job_city[df_job['WorkLocationCity'][i]],\n",
    "                                'job_exp_year': df_job['Value'][i]\n",
    "                                }\n",
    "\n",
    "\n",
    "        return job_info, job_city\n",
    "\n",
    "    def get_usr_info(self, path):\n",
    "        # 打开文件，读取所有行到data中\n",
    "        data = pd.read_excel(open(path, 'rb'),\n",
    "                             sheet_name='freelancer summary') \n",
    "        # 建立用户信息的字典\n",
    "        state_info, title_info, usr_info = {}, {}, {}\n",
    "        max_state_len, max_title_len= 0,0\n",
    "        count_s, count_t = 1,1\n",
    "        data['State'] = data['State'].fillna('')\n",
    "        data['Title'] = data['Title'].mask(pd.to_numeric(data['Title'], errors='coerce').notna()).fillna('')\n",
    "        data['Title'].replace(['/','& ','|','- ',', ','Sr.'],[' ',' ',' ',' ',' ','Senior'], inplace = True)\n",
    "        \n",
    "        \n",
    "        for i in data.index:\n",
    "            usr_id = data['FreelancerID'][i]\n",
    "            \n",
    "            each_title = data['Title'][i].split()\n",
    "            # print(each_title)\n",
    "            max_title_len = max(max_title_len, len(each_title))\n",
    "\n",
    "            if data['State'][i] not in state_info:\n",
    "                state_info[data['State'][i]] = count_s\n",
    "                count_s += 1\n",
    "\n",
    "            for t in each_title:\n",
    "                if t not in title_info:\n",
    "                    title_info[t] = count_t\n",
    "                    count_t +=1\n",
    "\n",
    "            v_title = [title_info[k] for k in each_title]\n",
    "            # print(usr_id,v_title)\n",
    "            while len(v_title)<39:\n",
    "                v_title.append(0)\n",
    "            # print(usr_id)\n",
    "            usr_info[usr_id] = {'usr_id': int(usr_id),\n",
    "                                 'title': v_title,\n",
    "                                 'state': state_info[data['State'][i]]\n",
    "\n",
    "                                }\n",
    "            self.max_usr_id = max(self.max_usr_id, int(usr_id))\n",
    "\n",
    "        return usr_info, title_info, state_info\n",
    "    # 得到评分数据\n",
    "    def get_rating_info(self, path):\n",
    "        # 打开文件，读取所有行到data中\n",
    "        df_rating = pd.read_excel(open(path, 'rb'))\n",
    "        # 创建一个字典\n",
    "        rating_info = {}\n",
    "        for i in df_rating.index:\n",
    "            usr_id, jobid, rating = df_rating['FreelancerID'][i], df_rating['jobid'][i],df_rating['rating'][i]\n",
    "            if usr_id in self.usr_info.keys():\n",
    "                if usr_id not in rating_info.keys():\n",
    "                    rating_info[usr_id] = {jobid:float(rating)}\n",
    "                else:\n",
    "                    rating_info[usr_id][jobid] = float(rating)\n",
    "        return rating_info\n",
    "    # 构建数据集\n",
    "    def get_dataset(self, usr_info, job_info, rating_info):\n",
    "        trainset = []\n",
    "        # 按照评分数据的key值索引数据\n",
    "        for usr_id in rating_info.keys():\n",
    "            usr_ratings = rating_info[usr_id]\n",
    "            for jobid in usr_ratings:\n",
    "                trainset.append({'usr_info': usr_info[usr_id],\n",
    "                                 'job_info': job_info[jobid],\n",
    "                                 'scores': usr_ratings[jobid]})\n",
    "        return trainset\n",
    "    \n",
    "    def load_data(self, dataset=None, mode='train'):\n",
    "        use_poster = False\n",
    "\n",
    "        # 定义数据迭代Batch大小\n",
    "        BATCHSIZE = 256\n",
    "\n",
    "        data_length = len(dataset)\n",
    "        index_list = list(range(data_length))\n",
    "        # 定义数据迭代加载器\n",
    "        def data_generator():\n",
    "            # 训练模式下，打乱训练数据\n",
    "            if mode == 'train':\n",
    "                random.shuffle(index_list)\n",
    "            # 声明每个特征的列表\n",
    "            usr_id_list,usr_title_list,usr_state_list,usr_job_list = [], [], [], []\n",
    "            job_id_list,job_city_list,job_exp_year_list = [], [], []\n",
    "            score_list = []\n",
    "            # 索引遍历输入数据集\n",
    "            for idx, i in enumerate(index_list):\n",
    "                # 获得特征数据保存到对应特征列表中\n",
    "                usr_id_list.append(dataset[i]['usr_info']['usr_id'])\n",
    "                usr_title_list.append(dataset[i]['usr_info']['title'])\n",
    "                usr_state_list.append(dataset[i]['usr_info']['state'])\n",
    "                # usr_job_list.append(dataset[i]['usr_info']['job'])\n",
    "\n",
    "                job_id_list.append(dataset[i]['job_info']['job_id'])\n",
    "                job_city_list.append(dataset[i]['job_info']['job_city'])\n",
    "                job_exp_year_list.append(dataset[i]['job_info']['job_exp_year'])\n",
    "                job_id = dataset[i]['job_info']['job_id']\n",
    "\n",
    "                if use_poster:\n",
    "                    # 不使用图像特征时，不读取图像数据，加快数据读取速度\n",
    "                    poster = Image.open(poster_path+'mov_id{}.jpg'.format(str(mov_id)))\n",
    "                    poster = poster.resize([64, 64])\n",
    "                    if len(poster.size) <= 2:\n",
    "                        poster = poster.convert(\"RGB\")\n",
    "\n",
    "                    mov_poster_list.append(np.array(poster))\n",
    "\n",
    "                score_list.append(int(dataset[i]['scores']))\n",
    "                # 如果读取的数据量达到当前的batch大小，就返回当前批次\n",
    "                \n",
    "                if len(usr_id_list)==BATCHSIZE:\n",
    "                    # 转换列表数据为数组形式，reshape到固定形状\n",
    "                    usr_id_arr = np.array(usr_id_list)\n",
    "                    usr_title_arr = np.reshape(np.array(usr_title_list), [BATCHSIZE, 1, 39]).astype(np.int64)\n",
    "                    usr_state_arr = np.array(usr_state_list)\n",
    "                    # usr_job_arr = np.array(usr_job_list)\n",
    "\n",
    "                    job_id_arr = np.array(job_id_list)\n",
    "                    job_city_arr = np.array(job_city_list)\n",
    "                    job_exp_year_arr = np.array(job_exp_year_list)\n",
    "\n",
    "                    if use_poster:\n",
    "                        mov_poster_arr = np.reshape(np.array(mov_poster_list)/127.5 - 1, [BATCHSIZE, 3, 64, 64]).astype(np.float32)\n",
    "                    else:\n",
    "                        mov_poster_arr = np.array([0.])\n",
    "\n",
    "                    scores_arr = np.reshape(np.array(score_list), [-1, 1]).astype(np.float32)\n",
    "\n",
    "                    # 返回当前批次数据\n",
    "                    yield [usr_id_arr, usr_title_arr, usr_state_arr], \\\n",
    "                           [job_id_arr, job_city_arr, job_exp_year_arr], scores_arr\n",
    "\n",
    "                    # 清空数据\n",
    "                    usr_id_list, usr_title_list, usr_state_list = [], [], []\n",
    "                    job_id_list,job_city_list,job_exp_year_list = [], [], []\n",
    "                    score_list = []\n",
    "                    mov_poster_list = []\n",
    "        return data_generator\n",
    "\n",
    "    # 声明数据读取类\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MovieLen(False)\n",
    "\n",
    "\n",
    "# 定义数据读取器\n",
    "train_loader = dataset.load_data(dataset=dataset.train_dataset, mode='train')\n",
    "# 迭代的读取数据， Batchsize = 256\n",
    "for idx, data in enumerate(train_loader()):\n",
    "    usr, job, score = data\n",
    "    print(\"打印用户ID，title,state，数据的维度：\")\n",
    "    for v in usr:\n",
    "        print(v.shape)\n",
    "    print(\"打印jobID，名字，类别数据的维度：\")\n",
    "    for v in job:\n",
    "        print(v.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Model(paddle.nn.Layer):\n",
    "    def __init__(self,use_poster, use_usr_title, use_usr_state, use_job_city, use_job_exp_year,fc_sizes):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # 将传入的name信息和bool型参数添加到模型类中\n",
    "        self.use_mov_poster = use_poster\n",
    "        self.use_usr_title = use_usr_title\n",
    "        self.use_usr_state = use_usr_state\n",
    "        self.use_job_city = use_job_city\n",
    "        self.use_job_exp_year = use_job_exp_year\n",
    "        self.fc_sizes = fc_sizes\n",
    "        \n",
    "        # 获取数据集的信息，并构建训练和验证集的数据迭代器\n",
    "        Dataset = MovieLen(self.use_mov_poster)\n",
    "        self.Dataset = Dataset\n",
    "        self.trainset = self.Dataset.train_dataset\n",
    "        self.valset = self.Dataset.valid_dataset\n",
    "        self.train_loader = self.Dataset.load_data(dataset=self.trainset, mode='train')\n",
    "        self.valid_loader = self.Dataset.load_data(dataset=self.valset, mode='valid')\n",
    "\n",
    "        \"\"\" define network layer for embedding usr info \"\"\"\n",
    "        USR_ID_NUM = Dataset.max_usr_id + 1\n",
    "        # 对用户ID做映射，并紧接着一个Linear层\n",
    "        self.usr_emb = Embedding(num_embeddings=USR_ID_NUM, embedding_dim=32, sparse=False)\n",
    "        self.usr_fc = Linear(in_features=32, out_features=32)\n",
    "        \n",
    "        # 对usr title 信息做映射，并紧接着一个Linear层\n",
    "        USR_TITLE_DICT_SIZE = len(Dataset.usr_title_info) + 1\n",
    "        self.usr_title_emb = Embedding(num_embeddings=USR_TITLE_DICT_SIZE, embedding_dim=32, sparse=False)\n",
    "        self.usr_title_conv = Conv2D(in_channels=1, out_channels=1, kernel_size=(3, 1), stride=(2,1), padding=0)\n",
    "        self.usr_title_conv2 = Conv2D(in_channels=1, out_channels=1, kernel_size=(3, 1), stride=1, padding=0)\n",
    "        \n",
    "        # 对usr state信息做映射，并紧接着一个Linear层\n",
    "        USR_STATE_DICT_SIZE = Dataset.max_usr_state + 1\n",
    "        self.usr_state_emb = Embedding(num_embeddings=USR_STATE_DICT_SIZE, embedding_dim=16)\n",
    "        self.usr_state_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # # 对用户职业信息做映射，并紧接着一个Linear层\n",
    "        # USR_JOB_DICT_SIZE = Dataset.max_usr_job + 1\n",
    "        # self.usr_job_emb = Embedding(num_embeddings=USR_JOB_DICT_SIZE, embedding_dim=16)\n",
    "        # self.usr_job_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # 新建一个Linear层，用于整合用户数据信息\n",
    "        self.usr_combined = Linear(in_features=80, out_features=200)\n",
    "        \n",
    "        \"\"\" define network layer for embedding job info \"\"\"\n",
    "        # 对电影ID信息做映射，并紧接着一个Linear层\n",
    "        JOB_DICT_SIZE = Dataset.max_job_id + 1\n",
    "        self.job_emb = Embedding(num_embeddings=JOB_DICT_SIZE, embedding_dim=32)\n",
    "        self.job_fc = Linear(in_features=32, out_features=32)\n",
    "        \n",
    "        # 对电影类别做映射\n",
    "        JOB_CITY_DICT_SIZE = Dataset.max_job_city + 1\n",
    "        self.job_city_emb = Embedding(num_embeddings=JOB_CITY_DICT_SIZE, embedding_dim=16, sparse=False)\n",
    "        self.job_city_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # 对电影名称做映射\n",
    "        JOB_EXP_YEAR_DICT_SIZE = 9 + 1\n",
    "        self.job_exp_year_emb = Embedding(num_embeddings=JOB_EXP_YEAR_DICT_SIZE, embedding_dim=16, sparse=False)\n",
    "        self.job_exp_year_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # 新建一个FC层，用于整合电影特征\n",
    "        self.job_concat_embed = Linear(in_features=64, out_features=200)\n",
    "\n",
    "        user_sizes = [200] + self.fc_sizes\n",
    "        acts = [\"relu\" for _ in range(len(self.fc_sizes))]\n",
    "        self._user_layers = []\n",
    "        for i in range(len(self.fc_sizes)):\n",
    "            linear = paddle.nn.Linear(\n",
    "                in_features=user_sizes[i],\n",
    "                out_features=user_sizes[i + 1],\n",
    "                weight_attr=paddle.ParamAttr(\n",
    "                    initializer=paddle.nn.initializer.Normal(\n",
    "                        std=1.0 / math.sqrt(user_sizes[i]))))\n",
    "            self._user_layers.append(linear)\n",
    "            if acts[i] == 'relu':\n",
    "                act = paddle.nn.ReLU()\n",
    "                self._user_layers.append(act)\n",
    "        \n",
    "        #电影特征和用户特征使用了不同的全连接层，不共享参数\n",
    "        job_sizes = [200] + self.fc_sizes\n",
    "        acts = [\"relu\" for _ in range(len(self.fc_sizes))]\n",
    "        self._job_layers = []\n",
    "        for i in range(len(self.fc_sizes)):\n",
    "            linear = paddle.nn.Linear(\n",
    "                in_features=job_sizes[i],\n",
    "                out_features=job_sizes[i + 1],\n",
    "                weight_attr=paddle.ParamAttr(\n",
    "                    initializer=paddle.nn.initializer.Normal(\n",
    "                        std=1.0 / math.sqrt(job_sizes[i]))))\n",
    "            self._job_layers.append(linear)\n",
    "            if acts[i] == 'relu':\n",
    "                act = paddle.nn.ReLU()\n",
    "                self._job_layers.append(act)\n",
    "        \n",
    "    # 定义计算用户特征的前向运算过程\n",
    "    def get_usr_feat(self, usr_var):\n",
    "        \"\"\" get usr features\"\"\"\n",
    "        # 获取到用户数据\n",
    "        usr_id, usr_title, usr_state = usr_var\n",
    "        # 将用户的ID数据经过embedding和Linear计算，得到的特征保存在feats_collect中\n",
    "        feats_collect = []\n",
    "        batch_size = usr_id.shape[0]\n",
    "        usr_id = self.usr_emb(usr_id)\n",
    "        usr_id = self.usr_fc(usr_id)\n",
    "        usr_id = F.relu(usr_id)\n",
    "        feats_collect.append(usr_id)\n",
    "\n",
    "        \n",
    "        if self.use_usr_title:\n",
    "            # 计算电影名字的特征映射，对特征映射使用卷积计算最终的特征\n",
    "            usr_title = self.usr_title_emb(usr_title)\n",
    "            usr_title = F.relu(self.usr_title_conv2(F.relu(self.usr_title_conv(usr_title))))\n",
    "            usr_title = paddle.sum(usr_title, axis=2, keepdim=False)\n",
    "            usr_title = F.relu(usr_title)\n",
    "            usr_title = paddle.reshape(usr_title, [batch_size, -1])\n",
    "            feats_collect.append(usr_title)\n",
    "        \n",
    "        \n",
    "        # 选择是否使用用户的年龄-职业特征\n",
    "        if self.use_usr_state:\n",
    "            # 计算用户的年龄特征，并保存在feats_collect中\n",
    "            usr_state = self.usr_state_emb(usr_state)\n",
    "            usr_state = self.usr_state_fc(usr_state)\n",
    "            usr_state = F.relu(usr_state)\n",
    "            feats_collect.append(usr_state)\n",
    "        \n",
    "        # 将用户的特征级联，并通过Linear层得到最终的用户特征\n",
    "        usr_feat = paddle.concat(feats_collect, axis=1)\n",
    "        user_features = F.tanh(self.usr_combined(usr_feat))\n",
    "        #通过3层全连接层，获得用于计算相似度的用户特征\n",
    "        for n_layer in self._user_layers:\n",
    "            user_features = n_layer(user_features)\n",
    "\n",
    "        return user_features\n",
    "\n",
    "        # 定义电影特征的前向计算过程\n",
    "    def get_job_feat(self, job_var):\n",
    "        \"\"\" get movie features\"\"\"\n",
    "        # 获得电影数据\n",
    "        job_id, job_city, job_exp_year= job_var\n",
    "        feats_collect = []\n",
    "        # 获得batchsize的大小\n",
    "        batch_size = job_id.shape[0]\n",
    "        # 计算电影ID的特征，并存在feats_collect中\n",
    "        job_id = self.job_emb(job_id)\n",
    "        job_id = self.job_fc(job_id)\n",
    "        job_id = F.relu(job_id)\n",
    "        feats_collect.append(job_id)\n",
    "        \n",
    "        # 如果使用电影的种类数据，计算电影种类特征的映射\n",
    "        if self.use_job_city:\n",
    "            # 计算电影种类的特征映射，对多个种类的特征求和得到最终特征\n",
    "            job_city = self.job_city_emb(job_city)\n",
    "            job_city = self.job_city_fc(job_city)\n",
    "            job_city = F.relu(job_city)\n",
    "            feats_collect.append(job_city)\n",
    "\n",
    "        if self.use_job_exp_year:\n",
    "            # 计算电影名字的特征映射，对特征映射使用卷积计算最终的特征\n",
    "            job_exp_year = self.job_exp_year_emb(job_exp_year)\n",
    "            job_exp_year = self.job_exp_year_fc(job_exp_year)\n",
    "            job_exp_year = F.relu(job_exp_year)\n",
    "            feats_collect.append(job_exp_year)\n",
    "            \n",
    "        # 使用一个全连接层，整合所有电影特征，映射为一个200维的特征向量\n",
    "        job_feat = paddle.concat(feats_collect, axis=1)\n",
    "        job_features = F.tanh(self.job_concat_embed(job_feat))\n",
    "        #通过3层全连接层，获得用于计算相似度的电影特征\n",
    "        for n_layer in self._job_layers:\n",
    "            job_features = n_layer(job_features)\n",
    "\n",
    "        return job_features\n",
    "    \n",
    "    # 定义个性化推荐算法的前向计算\n",
    "    def forward(self, usr_var, job_var):\n",
    "        # 计算用户特征和电影特征\n",
    "        user_features = self.get_usr_feat(usr_var)\n",
    "        job_features = self.get_job_feat(job_var)\n",
    "        # print(user_features.shape, job_features.shape)\n",
    "\n",
    "        # 根据计算的特征计算相似度\n",
    "        res = F.common.cosine_similarity(user_features, job_features).reshape([-1,1])\n",
    "        # 将相似度扩大范围到和电影评分相同数据范围\n",
    "        # res = paddle.scale(res, scale=5)\n",
    "        return user_features, job_features, res\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试电影特征提取网络\n",
    "fc_sizes=[128, 64, 32]\n",
    "model = Model(use_poster=False,use_usr_title=True, use_usr_state=True, use_job_city=True, use_job_exp_year=True,fc_sizes=fc_sizes)\n",
    "model.eval()\n",
    "\n",
    "data_loader = model.train_loader\n",
    "\n",
    "for idx, data in enumerate(data_loader()):\n",
    "    # 获得数据，并转为动态图格式\n",
    "    usr, job, score = data\n",
    "    # 只使用每个Batch的第一条数据\n",
    "    job_v = [var[0:1] for var in job]\n",
    "    # print(job_v)\n",
    "    _job_v = [np.squeeze(var[0:1]) for var in job]\n",
    "    print(\"输入的JobID数据：{}\\n城市数据：{} \\n年限需求数据：{} \".format(*_job_v))\n",
    "    job_v = [paddle.to_tensor(var) for var in job_v]\n",
    "    job_feat = model.get_job_feat(job_v)\n",
    "    print(\"计算得到的电影特征维度是：\", job_feat.shape)\n",
    "    \n",
    "    # 只使用每个Batch的第一条数据\n",
    "#     usr_v = [var[0:1] for var in usr]\n",
    "    \n",
    "#     _usr_v = [np.squeeze(var[0:1]) for var in usr]\n",
    "#     print(\"输入的usrID数据：{}\\nTITLE数据：{} \\nSTATE需求数据：{} \".format(*_usr_v))\n",
    "#     usr_v = [paddle.to_tensor(usr) for var in usr_v]\n",
    "#     usr_feat = model.get_usr_feat(usr_v)\n",
    "#     print(\"计算得到的usr特征维度是：\", usr_feat.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(data_loader()):\n",
    "    # 获得数据，并转为动态图格式\n",
    "    usr, job, score = data\n",
    "    usr_v = [var[0:1] for var in usr]\n",
    "    print(usr_v)\n",
    "\n",
    "    _usr_v = [np.squeeze(var[0:1]) for var in usr]\n",
    "    print(\"输入的JobID数据：{}\\n城市数据：{} \\n年限需求数据：{} \".format(*_usr_v))\n",
    "    usr_v = [paddle.to_tensor(var) for var in usr_v]\n",
    "    usr_feat = model.get_usr_feat(usr_v)\n",
    "    print(\"计算得到的电影特征维度是：\", usr_feat.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.common.cosine_similarity(usr_feat, job_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # 配置训练参数\n",
    "    lr = 0.001\n",
    "    Epoches = 20\n",
    "    paddle.set_device('cpu') \n",
    "\n",
    "    # 启动训练\n",
    "    model.train()\n",
    "    # 获得数据读取器\n",
    "    data_loader = model.train_loader\n",
    "    # 使用adam优化器，学习率使用0.01\n",
    "    opt = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "    \n",
    "    for epoch in range(0, Epoches):\n",
    "        for idx, data in enumerate(data_loader()):\n",
    "            # 获得数据，并转为tensor格式\n",
    "            usr, mov, score = data\n",
    "            usr_v = [paddle.to_tensor(var) for var in usr]\n",
    "            mov_v = [paddle.to_tensor(var) for var in mov]\n",
    "            scores_label = paddle.to_tensor(score)\n",
    "            # 计算出算法的前向计算结果\n",
    "            _, _, scores_predict = model(usr_v, mov_v)\n",
    "            # 计算loss\n",
    "            loss = F.square_error_cost(scores_predict, scores_label)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, idx, avg_loss.numpy()))\n",
    "                \n",
    "            # 损失函数下降，并清除梯度\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # 每个epoch 保存一次模型\n",
    "        paddle.save(model.state_dict(), './checkpoint/epoch'+str(epoch)+'.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Total dataset instances:  20024\n",
      "##MovieLens dataset information: \n",
      "usr num: 63025\n",
      "job num: 8199\n",
      "epoch: 0, batch_id: 0, loss is: [0.27118278]\n",
      "epoch: 0, batch_id: 10, loss is: [0.24645521]\n",
      "epoch: 0, batch_id: 20, loss is: [0.21586812]\n",
      "epoch: 0, batch_id: 30, loss is: [0.20636056]\n",
      "epoch: 0, batch_id: 40, loss is: [0.22544795]\n",
      "epoch: 0, batch_id: 50, loss is: [0.20607668]\n",
      "epoch: 0, batch_id: 60, loss is: [0.19542462]\n",
      "epoch: 1, batch_id: 0, loss is: [0.15241402]\n",
      "epoch: 1, batch_id: 10, loss is: [0.14415714]\n",
      "epoch: 1, batch_id: 20, loss is: [0.16082124]\n",
      "epoch: 1, batch_id: 30, loss is: [0.16417757]\n",
      "epoch: 1, batch_id: 40, loss is: [0.15804133]\n",
      "epoch: 1, batch_id: 50, loss is: [0.12346265]\n",
      "epoch: 1, batch_id: 60, loss is: [0.18682002]\n",
      "epoch: 2, batch_id: 0, loss is: [0.1163563]\n",
      "epoch: 2, batch_id: 10, loss is: [0.10856698]\n",
      "epoch: 2, batch_id: 20, loss is: [0.09622841]\n",
      "epoch: 2, batch_id: 30, loss is: [0.11619576]\n",
      "epoch: 2, batch_id: 40, loss is: [0.11683284]\n",
      "epoch: 2, batch_id: 50, loss is: [0.14227332]\n",
      "epoch: 2, batch_id: 60, loss is: [0.12071265]\n",
      "epoch: 3, batch_id: 0, loss is: [0.09030321]\n",
      "epoch: 3, batch_id: 10, loss is: [0.07199265]\n",
      "epoch: 3, batch_id: 20, loss is: [0.07072127]\n",
      "epoch: 3, batch_id: 30, loss is: [0.0882881]\n",
      "epoch: 3, batch_id: 40, loss is: [0.09992456]\n",
      "epoch: 3, batch_id: 50, loss is: [0.12695089]\n",
      "epoch: 3, batch_id: 60, loss is: [0.1159457]\n",
      "epoch: 4, batch_id: 0, loss is: [0.06129602]\n",
      "epoch: 4, batch_id: 10, loss is: [0.06842737]\n",
      "epoch: 4, batch_id: 20, loss is: [0.0768571]\n",
      "epoch: 4, batch_id: 30, loss is: [0.07609503]\n",
      "epoch: 4, batch_id: 40, loss is: [0.06295238]\n",
      "epoch: 4, batch_id: 50, loss is: [0.06391987]\n",
      "epoch: 4, batch_id: 60, loss is: [0.08424688]\n",
      "epoch: 5, batch_id: 0, loss is: [0.04277799]\n",
      "epoch: 5, batch_id: 10, loss is: [0.05446686]\n",
      "epoch: 5, batch_id: 20, loss is: [0.05114799]\n",
      "epoch: 5, batch_id: 30, loss is: [0.0596307]\n",
      "epoch: 5, batch_id: 40, loss is: [0.05648605]\n",
      "epoch: 5, batch_id: 50, loss is: [0.06214899]\n",
      "epoch: 5, batch_id: 60, loss is: [0.07248416]\n",
      "epoch: 6, batch_id: 0, loss is: [0.04365343]\n",
      "epoch: 6, batch_id: 10, loss is: [0.03832662]\n",
      "epoch: 6, batch_id: 20, loss is: [0.03874183]\n",
      "epoch: 6, batch_id: 30, loss is: [0.04656196]\n",
      "epoch: 6, batch_id: 40, loss is: [0.04355064]\n",
      "epoch: 6, batch_id: 50, loss is: [0.04143734]\n",
      "epoch: 6, batch_id: 60, loss is: [0.04437571]\n",
      "epoch: 7, batch_id: 0, loss is: [0.03321962]\n",
      "epoch: 7, batch_id: 10, loss is: [0.03892677]\n",
      "epoch: 7, batch_id: 20, loss is: [0.04447754]\n",
      "epoch: 7, batch_id: 30, loss is: [0.0383411]\n",
      "epoch: 7, batch_id: 40, loss is: [0.04017213]\n",
      "epoch: 7, batch_id: 50, loss is: [0.0408782]\n",
      "epoch: 7, batch_id: 60, loss is: [0.03574152]\n",
      "epoch: 8, batch_id: 0, loss is: [0.02625164]\n",
      "epoch: 8, batch_id: 10, loss is: [0.0240083]\n",
      "epoch: 8, batch_id: 20, loss is: [0.02910976]\n",
      "epoch: 8, batch_id: 30, loss is: [0.03714815]\n",
      "epoch: 8, batch_id: 40, loss is: [0.03021543]\n",
      "epoch: 8, batch_id: 50, loss is: [0.03904999]\n",
      "epoch: 8, batch_id: 60, loss is: [0.03837803]\n",
      "epoch: 9, batch_id: 0, loss is: [0.01893399]\n",
      "epoch: 9, batch_id: 10, loss is: [0.02948258]\n",
      "epoch: 9, batch_id: 20, loss is: [0.02449863]\n",
      "epoch: 9, batch_id: 30, loss is: [0.03167807]\n",
      "epoch: 9, batch_id: 40, loss is: [0.02760717]\n",
      "epoch: 9, batch_id: 50, loss is: [0.03335677]\n",
      "epoch: 9, batch_id: 60, loss is: [0.04702978]\n",
      "epoch: 10, batch_id: 0, loss is: [0.02454234]\n",
      "epoch: 10, batch_id: 10, loss is: [0.01851095]\n",
      "epoch: 10, batch_id: 20, loss is: [0.02383219]\n",
      "epoch: 10, batch_id: 30, loss is: [0.02464586]\n",
      "epoch: 10, batch_id: 40, loss is: [0.03624086]\n",
      "epoch: 10, batch_id: 50, loss is: [0.02671915]\n",
      "epoch: 10, batch_id: 60, loss is: [0.03368995]\n",
      "epoch: 11, batch_id: 0, loss is: [0.01712971]\n",
      "epoch: 11, batch_id: 10, loss is: [0.02222543]\n",
      "epoch: 11, batch_id: 20, loss is: [0.0233257]\n",
      "epoch: 11, batch_id: 30, loss is: [0.02296543]\n",
      "epoch: 11, batch_id: 40, loss is: [0.02352215]\n",
      "epoch: 11, batch_id: 50, loss is: [0.025971]\n",
      "epoch: 11, batch_id: 60, loss is: [0.02981907]\n",
      "epoch: 12, batch_id: 0, loss is: [0.02007116]\n",
      "epoch: 12, batch_id: 10, loss is: [0.01852352]\n",
      "epoch: 12, batch_id: 20, loss is: [0.01529807]\n",
      "epoch: 12, batch_id: 30, loss is: [0.0241323]\n",
      "epoch: 12, batch_id: 40, loss is: [0.02392272]\n",
      "epoch: 12, batch_id: 50, loss is: [0.02491525]\n",
      "epoch: 12, batch_id: 60, loss is: [0.02663371]\n",
      "epoch: 13, batch_id: 0, loss is: [0.01278874]\n",
      "epoch: 13, batch_id: 10, loss is: [0.01764992]\n",
      "epoch: 13, batch_id: 20, loss is: [0.01328154]\n",
      "epoch: 13, batch_id: 30, loss is: [0.01854019]\n",
      "epoch: 13, batch_id: 40, loss is: [0.01642909]\n",
      "epoch: 13, batch_id: 50, loss is: [0.02581256]\n",
      "epoch: 13, batch_id: 60, loss is: [0.02578427]\n",
      "epoch: 14, batch_id: 0, loss is: [0.01372346]\n",
      "epoch: 14, batch_id: 10, loss is: [0.0211309]\n",
      "epoch: 14, batch_id: 20, loss is: [0.01333532]\n",
      "epoch: 14, batch_id: 30, loss is: [0.01535286]\n",
      "epoch: 14, batch_id: 40, loss is: [0.01576764]\n",
      "epoch: 14, batch_id: 50, loss is: [0.02103766]\n",
      "epoch: 14, batch_id: 60, loss is: [0.02618024]\n",
      "epoch: 15, batch_id: 0, loss is: [0.01606742]\n",
      "epoch: 15, batch_id: 10, loss is: [0.01595316]\n",
      "epoch: 15, batch_id: 20, loss is: [0.01107375]\n",
      "epoch: 15, batch_id: 30, loss is: [0.01270083]\n",
      "epoch: 15, batch_id: 40, loss is: [0.01412396]\n",
      "epoch: 15, batch_id: 50, loss is: [0.01778806]\n",
      "epoch: 15, batch_id: 60, loss is: [0.01995137]\n",
      "epoch: 16, batch_id: 0, loss is: [0.01623319]\n",
      "epoch: 16, batch_id: 10, loss is: [0.01324467]\n",
      "epoch: 16, batch_id: 20, loss is: [0.01930222]\n",
      "epoch: 16, batch_id: 30, loss is: [0.01481911]\n",
      "epoch: 16, batch_id: 40, loss is: [0.01391287]\n",
      "epoch: 16, batch_id: 50, loss is: [0.01681965]\n",
      "epoch: 16, batch_id: 60, loss is: [0.0157715]\n",
      "epoch: 17, batch_id: 0, loss is: [0.01174786]\n",
      "epoch: 17, batch_id: 10, loss is: [0.00946745]\n",
      "epoch: 17, batch_id: 20, loss is: [0.01828673]\n",
      "epoch: 17, batch_id: 30, loss is: [0.01792111]\n",
      "epoch: 17, batch_id: 40, loss is: [0.01272144]\n",
      "epoch: 17, batch_id: 50, loss is: [0.02057538]\n",
      "epoch: 17, batch_id: 60, loss is: [0.01560753]\n",
      "epoch: 18, batch_id: 0, loss is: [0.00814061]\n",
      "epoch: 18, batch_id: 10, loss is: [0.00966531]\n",
      "epoch: 18, batch_id: 20, loss is: [0.01213699]\n",
      "epoch: 18, batch_id: 30, loss is: [0.01451177]\n",
      "epoch: 18, batch_id: 40, loss is: [0.01274408]\n",
      "epoch: 18, batch_id: 50, loss is: [0.0122211]\n",
      "epoch: 18, batch_id: 60, loss is: [0.01822764]\n",
      "epoch: 19, batch_id: 0, loss is: [0.00747028]\n",
      "epoch: 19, batch_id: 10, loss is: [0.01626372]\n",
      "epoch: 19, batch_id: 20, loss is: [0.01471166]\n",
      "epoch: 19, batch_id: 30, loss is: [0.01606155]\n",
      "epoch: 19, batch_id: 40, loss is: [0.01313619]\n",
      "epoch: 19, batch_id: 50, loss is: [0.01636737]\n",
      "epoch: 19, batch_id: 60, loss is: [0.01258285]\n"
     ]
    }
   ],
   "source": [
    "fc_sizes=[128, 64, 32]\n",
    "model = Model(use_poster=False,use_usr_title=True, use_usr_state=True, use_job_city=True, use_job_exp_year=True,fc_sizes=fc_sizes)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def evaluation(model, params_file_path):\n",
    "    model_state_dict = paddle.load(params_file_path)\n",
    "    model.load_dict(model_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    acc_set = []\n",
    "    avg_loss_set = []\n",
    "    squaredError=[]\n",
    "    for idx, data in enumerate(model.valid_loader()):\n",
    "        usr, mov, score_label = data\n",
    "        usr_v = [paddle.to_tensor(var) for var in usr]\n",
    "        mov_v = [paddle.to_tensor(var) for var in mov]\n",
    "\n",
    "        _, _, scores_predict = model(usr_v, mov_v)\n",
    "        pred_scores = scores_predict.numpy()\n",
    "        \n",
    "        avg_loss_set.append(np.mean(np.abs(pred_scores - score_label)))\n",
    "        squaredError.extend(np.abs(pred_scores - score_label)**2)\n",
    "\n",
    "        diff = np.abs(pred_scores - score_label)\n",
    "        diff[diff>0.5] = 1\n",
    "        acc = 1 - np.mean(diff)\n",
    "        acc_set.append(acc)\n",
    "    RMSE=sqrt(np.sum(squaredError) / len(squaredError))\n",
    "    \n",
    "    # print(\"RMSE = \", sqrt(np.sum(squaredError) / len(squaredError)))#均方根误差RMSE\n",
    "        \n",
    "    return np.mean(acc_set), np.mean(avg_loss_set),RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 0 ACC: 0.49630554233278545 MAE: 0.3990512 RMSE: 0.4518680144295593\n",
      "no: 1 ACC: 0.5179521186011178 MAE: 0.38599536 RMSE: 0.46779810211569234\n",
      "no: 2 ACC: 0.536872216633388 MAE: 0.37615007 RMSE: 0.4761036935701747\n",
      "no: 3 ACC: 0.5462229081562587 MAE: 0.37712243 RMSE: 0.4950257897092257\n",
      "no: 4 ACC: 0.5516033172607422 MAE: 0.37069663 RMSE: 0.50065746677657\n",
      "no: 5 ACC: 0.5630768324647631 MAE: 0.3681319 RMSE: 0.5022743134732185\n",
      "no: 6 ACC: 0.5829110656465802 MAE: 0.3555691 RMSE: 0.4961206280836596\n",
      "no: 7 ACC: 0.5693751914160592 MAE: 0.3640159 RMSE: 0.4996352397930526\n",
      "no: 8 ACC: 0.5662841584001269 MAE: 0.36337328 RMSE: 0.49569641610324827\n",
      "no: 9 ACC: 0.5749909579753876 MAE: 0.3575831 RMSE: 0.49407045198905497\n",
      "no: 10 ACC: 0.5615896199430738 MAE: 0.36656183 RMSE: 0.4924385658769846\n",
      "no: 11 ACC: 0.5761820673942566 MAE: 0.35617608 RMSE: 0.49065357364549816\n",
      "no: 12 ACC: 0.5813469077859607 MAE: 0.35421118 RMSE: 0.48893387259501736\n",
      "no: 13 ACC: 0.5737074783870152 MAE: 0.35866183 RMSE: 0.48968861075066755\n",
      "no: 14 ACC: 0.5751145524638039 MAE: 0.3606995 RMSE: 0.49256674758911007\n",
      "no: 15 ACC: 0.5715915816170829 MAE: 0.36478958 RMSE: 0.4924982694411493\n",
      "no: 16 ACC: 0.5787366926670074 MAE: 0.35958678 RMSE: 0.4942143028296608\n",
      "no: 17 ACC: 0.5717370850699288 MAE: 0.36652628 RMSE: 0.49318462582548744\n",
      "no: 18 ACC: 0.573183753660747 MAE: 0.36341217 RMSE: 0.4927889550665902\n",
      "no: 19 ACC: 0.5603920902524676 MAE: 0.37281877 RMSE: 0.49455947399417144\n"
     ]
    }
   ],
   "source": [
    "param_path = \"./checkpoint/epoch\"\n",
    "for i in range(20):\n",
    "    acc, mae,RMSE = evaluation(model, param_path+str(i)+'.pdparams')\n",
    "    print(\"no:\",i,\"ACC:\", acc, \"MAE:\", mae,'RMSE:',RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
